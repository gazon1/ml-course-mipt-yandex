{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import scipy.sparse as sp\n",
    "from collections import defaultdict, Counter\n",
    "from datetime import datetime\n",
    "from copy import deepcopy\n",
    "\n",
    "from pprint import pprint\n",
    "tqdm.tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '/mnt/data_volume/site2vec/jupyter_home/temp/vladtitov/rekko_challenge'\n",
    "\n",
    "def read_data(mode):\n",
    "    with open(os.path.join(DATA_PATH, 'test_users.json'), 'r') as f:\n",
    "        test_users = set(json.load(f)['users'])\n",
    "    \n",
    "    if mode == 'production':\n",
    "        predictions = dict()\n",
    "        for user_uid in test_users:\n",
    "            predictions[user_uid] = []\n",
    "            \n",
    "    with open(os.path.join(DATA_PATH, 'catalogue.json'), 'r') as f:\n",
    "        catalogue = json.load(f)    \n",
    "    catalogue = {int(k): v for k, v in catalogue.items()}\n",
    "    \n",
    "    catalogue_to_hahn = []\n",
    "    for element_uid in catalogue:\n",
    "        row = {'element_uid': element_uid}\n",
    "        for key in catalogue[element_uid]:\n",
    "            if key in {'feature_1', 'feature_2', 'feature_3', 'feature_4', 'feature_5'}:\n",
    "                row[key] = float(catalogue[element_uid][key])\n",
    "            else:\n",
    "                row[key] = catalogue[element_uid][key]\n",
    "        catalogue_to_hahn.append(row)\n",
    "    catalogue = pd.DataFrame(catalogue_to_hahn)\n",
    "    \n",
    "    transactions = pd.read_csv(\n",
    "        os.path.join(DATA_PATH, 'transactions.csv'),\n",
    "        dtype={\n",
    "            'element_uid': np.uint16,\n",
    "            'user_uid': np.uint32,\n",
    "            'consumption_mode': 'category',\n",
    "            'ts': np.float64,\n",
    "            'watched_time': np.uint64,\n",
    "            'device_type': np.uint8,\n",
    "            'device_manufacturer': np.uint8\n",
    "        }\n",
    "    )\n",
    "    transactions_with_catalogue = transactions.join(catalogue.set_index('element_uid'), how='inner', \n",
    "                                                    on='element_uid', sort='ts')\n",
    "    \n",
    "    validation_threshold = np.percentile(transactions_with_catalogue['ts'], 70)\n",
    "    if mode == 'testing':\n",
    "        transactions_with_catalogue_val = transactions_with_catalogue.loc[transactions_with_catalogue['ts'] > validation_threshold].copy()\n",
    "        transactions_with_catalogue = transactions_with_catalogue.loc[transactions_with_catalogue['ts'] <= validation_threshold]\n",
    "    transactions_with_catalogue['ts'] = transactions_with_catalogue['ts'].progress_apply(\n",
    "        lambda x: datetime.utcfromtimestamp(int(x * 60)))\n",
    "    \n",
    "    ratings = pd.read_csv(\n",
    "        os.path.join(DATA_PATH, 'ratings.csv'),\n",
    "        dtype={\n",
    "            'element_uid': np.uint16,\n",
    "            'user_uid': np.uint32,\n",
    "            'ts': np.float64,\n",
    "            'rating': np.uint8\n",
    "        }\n",
    "    )\n",
    "    if mode == 'testing':\n",
    "        ratings = ratings.loc[ratings['ts'] <= validation_threshold]\n",
    "    ratings['ts'] = ratings['ts'].progress_apply(lambda x: datetime.utcfromtimestamp(int(x * 60)))\n",
    "    \n",
    "    bookmarks = pd.read_csv(\n",
    "        os.path.join(DATA_PATH, 'bookmarks.csv'),\n",
    "        dtype={\n",
    "            'element_uid': np.uint16,\n",
    "            'user_uid': np.uint32,\n",
    "            'ts': np.float64\n",
    "        }\n",
    "    )\n",
    "\n",
    "    if mode == 'testing':\n",
    "        bookmarks = bookmarks.loc[bookmarks['ts'] <= validation_threshold]\n",
    "        \n",
    "    user_consumed_movies = defaultdict(set)\n",
    "    for user_uid, element_uid in tqdm.tqdm(transactions_with_catalogue.loc[:, ['user_uid', 'element_uid']].values):\n",
    "        user_consumed_movies[user_uid].add(element_uid)\n",
    "       \n",
    "    val_answers = None\n",
    "    if mode == 'testing':\n",
    "        predictions = dict()\n",
    "        val_answers = defaultdict(set)\n",
    "\n",
    "        for user_uid, element_uid in tqdm.tqdm(transactions_with_catalogue_val.loc[:, ['user_uid', 'element_uid']].values):\n",
    "            if user_uid in test_users: #and element_uid not in unavailable_movies:\n",
    "                val_answers[user_uid].add(element_uid)\n",
    "                predictions[user_uid] = []\n",
    "                \n",
    "    return test_users, predictions, transactions_with_catalogue, ratings, bookmarks, user_consumed_movies, val_answers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9643012/9643012 [00:29<00:00, 324350.76it/s]\n",
      "100%|██████████| 438790/438790 [00:01<00:00, 312645.07it/s]\n",
      "100%|██████████| 9643012/9643012 [00:24<00:00, 390544.78it/s]\n"
     ]
    }
   ],
   "source": [
    "data = read_data('production')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(data, model):\n",
    "    test_users, predictions, movie_score_by_user, top_movies, users_bookmarked_movies, user_consumed_movies, val_answers = data\n",
    "    \n",
    "    movie_score_by_user['user_uid'] = movie_score_by_user['user_uid'].astype('category')\n",
    "    movie_score_by_user['element_uid'] = movie_score_by_user['element_uid'].astype('category')\n",
    "    ratings_matrix = sp.coo_matrix(\n",
    "        (movie_score_by_user['rating'].astype(np.float64) + 1,\n",
    "            (\n",
    "                movie_score_by_user['element_uid'].cat.codes.copy(),\n",
    "                movie_score_by_user['user_uid'].cat.codes.copy()\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "    ratings_matrix = ratings_matrix.tocsr()\n",
    "    ratings_matrix_T = ratings_matrix.T.tocsr()\n",
    "    \n",
    "    model.fit(ratings_matrix)\n",
    "    \n",
    "    user_uid_to_cat = dict(zip(\n",
    "        movie_score_by_user['user_uid'].cat.categories,\n",
    "        range(len(movie_score_by_user['user_uid'].cat.categories))\n",
    "    ))\n",
    "\n",
    "    element_uid_to_cat = dict(zip(\n",
    "        movie_score_by_user['element_uid'].cat.categories,\n",
    "        range(len(movie_score_by_user['element_uid'].cat.categories))\n",
    "    ))\n",
    "    \n",
    "    filtered_elements_cat = {k: [element_uid_to_cat.get(x, None) for x in v] for k, v in user_consumed_movies.items()}\n",
    "    \n",
    "    model_predictions = defaultdict(list)  \n",
    "    val_answers = val_answers if val_answers is not None else test_users\n",
    "    for user_uid in tqdm.tqdm(val_answers):\n",
    "        # transform user_uid to model's internal user category\n",
    "        try:\n",
    "            user_cat = user_uid_to_cat[user_uid]\n",
    "        except LookupError:\n",
    "            continue\n",
    "    \n",
    "        # perform inference\n",
    "        recs = model.recommend(\n",
    "            user_cat,\n",
    "            ratings_matrix_T,\n",
    "            N=100,\n",
    "            filter_already_liked_items=True,\n",
    "            filter_items=filtered_elements_cat.get(user_uid, set())\n",
    "        )\n",
    "    \n",
    "        # drop scores and transform model's internal elelemnt category to element_uid for every prediction\n",
    "        # also convert np.uint64 to int so it could be json serialized later\n",
    "        model_predictions[user_uid] = [int(movie_score_by_user['element_uid'].cat.categories[i]) for i, _ in recs]\n",
    "        \n",
    "    for user_uid in tqdm.tqdm(val_answers):\n",
    "        for movie in model_predictions[user_uid]:\n",
    "            if len(predictions[user_uid]) == 100:\n",
    "                break\n",
    "            if movie in users_bookmarked_movies[user_uid] and movie not in user_consumed_movies[user_uid]: #and movie not in unavailable_movies:\n",
    "                predictions[user_uid].append(movie)\n",
    "                user_consumed_movies[user_uid].add(movie) \n",
    "            \n",
    "        for movie in users_bookmarked_movies[user_uid]:\n",
    "            if len(predictions[user_uid]) == 100:\n",
    "                break\n",
    "            if movie not in user_consumed_movies[user_uid]: #and movie not in unavailable_movies:\n",
    "                predictions[user_uid].append(movie)\n",
    "                user_consumed_movies[user_uid].add(movie) \n",
    "    \n",
    "        for movie in model_predictions[user_uid]:\n",
    "            if len(predictions[user_uid]) == 100:\n",
    "                break\n",
    "            if movie not in user_consumed_movies[user_uid]: #and movie not in unavailable_movies:\n",
    "                predictions[user_uid].append(movie)\n",
    "                user_consumed_movies[user_uid].add(movie)\n",
    "        \n",
    "        for movie in top_movies:\n",
    "            if len(predictions[user_uid]) == 100:\n",
    "                break\n",
    "            if movie not in user_consumed_movies[user_uid]: #and movie not in unavailable_movies:\n",
    "                predictions[user_uid].append(movie)\n",
    "                user_consumed_movies[user_uid].add(movie)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_rekko(data, smoothing, models_list):\n",
    "    test_users, predictions, transactions_with_catalogue, ratings, bookmarks, user_consumed_movies, val_answers = data\n",
    "    \n",
    "    max_date = max(transactions_with_catalogue['ts'])\n",
    "    transactions_with_catalogue['rating'] = pd.Series(np.full(len(transactions_with_catalogue), 10.0), \n",
    "                                                      index=transactions_with_catalogue.index)   \n",
    "    new_ratings = []\n",
    "    for rating, duration, watched_time, date, movie_type, consumption_mode in tqdm.tqdm(\n",
    "        transactions_with_catalogue.loc[:, ['rating', 'duration', 'watched_time', \n",
    "                                            'ts', 'type', 'consumption_mode']].values):\n",
    "        if consumption_mode in ('P', 'R'):\n",
    "            new_ratings.append(rating * smoothing**((max_date - date).days) + 0.00001)\n",
    "        elif movie_type == 'movie' and (watched_time + 1) / (duration * 60 + 1) >= 0.5:\n",
    "            new_ratings.append(rating * smoothing**((max_date - date).days) + 0.00001)\n",
    "        elif movie_type != 'movie' and (watched_time + 1) / (duration * 60 + 1) >= 2:\n",
    "            new_ratings.append(rating * smoothing**((max_date - date).days) + 0.00001)\n",
    "        else:\n",
    "            new_ratings.append(0.00001)\n",
    "    transactions_with_catalogue['rating'] = pd.Series(new_ratings, index=transactions_with_catalogue.index)\n",
    "    \n",
    "    new_ratings = []\n",
    "    for rating, date in tqdm.tqdm(ratings.loc[:, ['rating', 'ts']].values):\n",
    "        new_ratings.append(rating * smoothing**((max_date - date).days))\n",
    "    ratings['rating'] = pd.Series(new_ratings, index=ratings.index)\n",
    "    \n",
    "    movie_score_by_user = pd.concat([ratings.loc[:, ['element_uid', 'user_uid', 'rating']], \n",
    "                                    transactions_with_catalogue.loc[:, ['element_uid', 'user_uid', 'rating']]], \n",
    "                                    ignore_index=True).groupby(\n",
    "        ['element_uid', 'user_uid'], as_index=False).sum().sort_values(by=['user_uid'])\n",
    "                                     \n",
    "    movies_scores = movie_score_by_user.loc[:, ['element_uid', 'rating']].groupby(\n",
    "        'element_uid', as_index=False).sum().sort_values(by=['rating'], ascending=False)\n",
    "    top_movies = list(movies_scores['element_uid'])[:100]\n",
    "                                     \n",
    "    users_bookmarked_movies = defaultdict(list)\n",
    "    bookmarks_with_scores = bookmarks.join(movies_scores.set_index('element_uid'), how='inner', \n",
    "                                           on='element_uid', sort='element_uid')\n",
    "    for _, row in tqdm.tqdm(bookmarks_with_scores.iterrows()):\n",
    "        users_bookmarked_movies[row['user_uid']].append((row['element_uid'], row['rating']))\n",
    "\n",
    "    for user_uid in tqdm.tqdm(users_bookmarked_movies):\n",
    "        users_bookmarked_movies[user_uid] = list(map(lambda x: int(x[0]), sorted(users_bookmarked_movies[user_uid], \n",
    "                                                                                 key=lambda x: x[1], reverse=True)))\n",
    "    \n",
    "    print('Ready for models execution')\n",
    "                                     \n",
    "    result = []\n",
    "    data = (test_users, predictions, movie_score_by_user, top_movies, users_bookmarked_movies, \n",
    "            user_consumed_movies, val_answers)\n",
    "    model_num = 0\n",
    "    for model in models_list:\n",
    "        model_num += 1\n",
    "        result.append(evaluate_model(deepcopy(data), model))\n",
    "        print('Executed model number {}'.format(model_num))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9643012/9643012 [04:02<00:00, 39702.96it/s]\n",
      "100%|██████████| 438790/438790 [00:15<00:00, 28553.05it/s]\n",
      "921945it [01:52, 8182.70it/s]\n",
      "100%|██████████| 143833/143833 [00:01<00:00, 77985.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready for models execution\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8670/8670 [00:00<00:00, 14843.81it/s]\n",
      "100%|██████████| 50000/50000 [00:45<00:00, 1088.18it/s]\n",
      "100%|██████████| 50000/50000 [00:38<00:00, 1296.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executed model number 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8670/8670 [00:00<00:00, 14631.97it/s]\n",
      "100%|██████████| 50000/50000 [00:42<00:00, 1175.76it/s]\n",
      "100%|██████████| 50000/50000 [00:38<00:00, 1300.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executed model number 2\n"
     ]
    }
   ],
   "source": [
    "from implicit.nearest_neighbours import BM25Recommender, ItemItemRecommender, CosineRecommender\n",
    "\n",
    "result = []\n",
    "#result += evaluate_rekko(deepcopy(data), 0.995, [BM25Recommender(K=200), ItemItemRecommender(K=100), \n",
    "#                                                 CosineRecommender(K=100)])\n",
    "for smoothing in (0.999, 0.995, 0.99):\n",
    "    result += evaluate_rekko(deepcopy(data), smoothing, \n",
    "                             [BM25Recommender(K=200), ItemItemRecommender(K=200), CosineRecommender(K=100)])\n",
    "    print('Processed smoothing {}'.format(smoothing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "print(len(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnap_metric(predictions, answers, pred_size):\n",
    "    assert isinstance(predictions, dict) and isinstance(answers, dict), 'wrong type of input'\n",
    "    assert len(predictions) == len(answers), 'predictions and answers should have the same size'\n",
    "    \n",
    "    metric = 0.0\n",
    "    for user in predictions:\n",
    "        assert len(predictions[user]) == pred_size, 'len of user predictions should be {}'.format(pred_size)\n",
    "        already_answered = set()\n",
    "        total_consumed = min(pred_size, len(answers[user]))\n",
    "        pk_metric = 0.0\n",
    "        apk_metric = 0.0\n",
    "        for i in range(total_consumed):\n",
    "            if predictions[user][i] not in already_answered and predictions[user][i] in answers[user]:\n",
    "                pk_metric += 1\n",
    "                apk_metric += pk_metric / (i + 1)\n",
    "                already_answered.add(predictions[user][i])\n",
    "                \n",
    "        metric += apk_metric / total_consumed\n",
    "    \n",
    "    return metric / len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mnap_metric 0.03294918833669094 on smoothing 0.995 and model BM25Recommender\n",
      "mnap_metric 0.03451084029406736 on smoothing 0.995 and model ItemItemRecommender\n",
      "mnap_metric 0.03364825053036425 on smoothing 0.995 and model CosineRecommender\n"
     ]
    }
   ],
   "source": [
    "for j, model in enumerate(['BM25Recommender', 'ItemItemRecommender', 'CosineRecommender']):\n",
    "    index = j\n",
    "    predictions = dict()\n",
    "    for user_uid in result[index]:\n",
    "        predictions[user_uid] = result[index][user_uid][:20]\n",
    "    print('mnap_metric {} on smoothing {} and model {}'.format(mnap_metric(predictions, data[-1], 20), \n",
    "                                                                0.995, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = dict()\n",
    "for user_uid in result[0]:\n",
    "    predictions[user_uid] = result[0][user_uid][:20]\n",
    "len(predictions)\n",
    "#mnap_metric(predictions, data[-1], 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.034629372973859145 - ItemItem smoothing 0.995"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.03117753361301951 - ItemItem smoothing 0.98"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mnap_metric 0.024218662364930674 on smoothing 1 and model BM25Recommender\n",
      "mnap_metric 0.02115261107624082 on smoothing 1 and model ItemItemRecommender\n",
      "mnap_metric 0.024359310616094997 on smoothing 1 and model CosineRecommender\n",
      "mnap_metric 0.026675494240166988 on smoothing 0.999 and model BM25Recommender\n",
      "mnap_metric 0.026743781185005918 on smoothing 0.999 and model ItemItemRecommender\n",
      "mnap_metric 0.027536437111490335 on smoothing 0.999 and model CosineRecommender\n",
      "mnap_metric 0.028835989137372793 on smoothing 0.99 and model BM25Recommender\n",
      "mnap_metric 0.03328374125714043 on smoothing 0.99 and model ItemItemRecommender\n",
      "mnap_metric 0.030320677577801516 on smoothing 0.99 and model CosineRecommender\n"
     ]
    }
   ],
   "source": [
    "for i, smoothing in enumerate([1, 0.999, 0.99]):\n",
    "    for j, model in enumerate(['BM25Recommender', 'ItemItemRecommender', 'CosineRecommender']):\n",
    "        index = i * 3 + j\n",
    "        predictions = dict()\n",
    "        for user_uid in result[index]:\n",
    "            predictions[user_uid] = result[index][user_uid][:20]\n",
    "        print('mnap_metric {} on smoothing {} and model {}'.format(mnap_metric(predictions, data[-1], 20), \n",
    "                                                                   smoothing, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mnap_metric 0.02796821919267791 on smoothing 1 and model BM25Recommender\n",
      "mnap_metric 0.026855798901948606 on smoothing 1 and model ItemItemRecommender\n",
      "mnap_metric 0.02760982425865799 on smoothing 1 and model CosineRecommender\n",
      "mnap_metric 0.03294918833669094 on smoothing 0.999 and model BM25Recommender\n",
      "mnap_metric 0.03457980164216801 on smoothing 0.999 and model ItemItemRecommender\n",
      "mnap_metric 0.033591242868644004 on smoothing 0.999 and model CosineRecommender\n",
      "mnap_metric 0.031386265026930746 on smoothing 0.99 and model BM25Recommender\n",
      "mnap_metric 0.033382716111329085 on smoothing 0.99 and model ItemItemRecommender\n",
      "mnap_metric 0.030892794258313108 on smoothing 0.99 and model CosineRecommender\n"
     ]
    }
   ],
   "source": [
    "for i, smoothing in enumerate([1, 0.999, 0.99]):\n",
    "    for j, model in enumerate(['BM25Recommender', 'ItemItemRecommender', 'CosineRecommender']):\n",
    "        index = i * 3 + j\n",
    "        predictions = dict()\n",
    "        for user_uid in result[index]:\n",
    "            predictions[user_uid] = result[index][user_uid][:20]\n",
    "        print('mnap_metric {} on smoothing {} and model {}'.format(mnap_metric(predictions, data[-1], 20), \n",
    "                                                                   smoothing, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mix_solutions(result, rates, max_score, movies_num_to_leave):\n",
    "    scores = dict()\n",
    "    for user_uid in result[0]:\n",
    "        scores[user_uid] = Counter()\n",
    "        \n",
    "    for i, prediction in tqdm.tqdm(enumerate(result)):\n",
    "        for user_uid in prediction:\n",
    "            for pos, movie in enumerate(prediction[user_uid]):\n",
    "                scores[user_uid][movie] += rates[i] * max_score * 0.99**pos\n",
    "    \n",
    "    final_predictions = dict()\n",
    "    for user_uid in tqdm.tqdm(scores):\n",
    "        # print(scores[user_uid].values())\n",
    "        final_predictions[user_uid] = list(map(lambda x: x[0], \n",
    "                                              sorted(scores[user_uid].items(), \n",
    "                                                     key=lambda x: x[1], reverse=True)))[:movies_num_to_leave]\n",
    "    return final_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = [\n",
    "    1 * 4, 1 * 3, 1 * 4,\n",
    "    6 * 3, 9 * 4, 6 * 4,\n",
    "    5 * 3, 9 * 3, 4 * 3\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [00:38,  4.22s/it]\n",
      "100%|██████████| 50000/50000 [00:05<00:00, 9620.33it/s]\n"
     ]
    }
   ],
   "source": [
    "predictions = mix_solutions(result, ratings, 100, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.037033576680949795"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnap_metric(predictions, data[-1], 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ratings = [\n",
    "#    1 * 4, 4 * 4, 4 * 4,\n",
    "#    3 * 3, 6 * 4, 9 * 4,\n",
    "#    1 * 3, 6 * 4, 1 * 3\n",
    "#] - best public"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = [\n",
    "    1 * 4, 1 * 3, 1 * 4,\n",
    "    6 * 3, 9 * 4, 6 * 4,\n",
    "    5 * 3, 9 * 3, 4 * 3\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03705185079081703"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnap_metric(predictions, data[-1], 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ratings = [\n",
    "#    0 * 3, 1 * 4, 1 * 4,\n",
    "#    3 * 3, 9 * 4, 6 * 4,\n",
    "#    3 * 3, 9 * 3, 4 * 3\n",
    "#]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03586838670807896"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnap_metric(predictions, data[-1], 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Записываем ответ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(predictions) == 50000\n",
    "with open('bookmarks_bm25_cosine_knearest_feated_hack_0999_0995_099_decay_best_val_mix.json', 'w') as f:\n",
    "    json.dump(predictions, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Неудачные попытки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_users_not_using_bookmarks(data):\n",
    "    test_users, transactions_with_catalogue, bookmarks = data\n",
    "    validation_threshold = np.percentile(transactions_with_catalogue['ts'], 75)\n",
    "    \n",
    "    transactions_with_catalogue_val = transactions_with_catalogue.loc[transactions_with_catalogue['ts'] > validation_threshold].copy()\n",
    "    transactions_with_catalogue = transactions_with_catalogue.loc[transactions_with_catalogue['ts'] <= validation_threshold]\n",
    "    bookmarks = bookmarks.loc[bookmarks['ts'] <= validation_threshold]\n",
    "    \n",
    "    users_bookmarked_movies = defaultdict(set)\n",
    "    for _, row in tqdm.tqdm(bookmarks.iterrows()):\n",
    "        users_bookmarked_movies[row['user_uid']].add(row['element_uid'])\n",
    "        \n",
    "    val_answers = defaultdict(set)\n",
    "    for user_uid, element_uid in tqdm.tqdm(transactions_with_catalogue_val.loc[:, ['user_uid', 'element_uid']].values):\n",
    "        if user_uid in test_users:\n",
    "            val_answers[user_uid].add(element_uid)\n",
    "\n",
    "    users_not_using_bookmarks = set()\n",
    "    for user_uid in val_answers:\n",
    "        if len(users_bookmarked_movies[user_uid]) > 0 and \\\n",
    "            len(users_bookmarked_movies[user_uid].intersection(val_answers[user_uid])) == 0:\n",
    "            users_not_using_bookmarks.add(user_uid)\n",
    "    return users_not_using_bookmarks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
